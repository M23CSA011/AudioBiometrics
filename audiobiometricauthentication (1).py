# -*- coding: utf-8 -*-
"""AudioBiometricAuthentication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1txj0WhmaiCBcoPiFdDvWG_z_6Li4m95Q
"""

import numpy as np
import librosa
import os
import pandas as pd
import pickle
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import torch
import torchaudio
import torchaudio.transforms as transforms
import pandas as pd
import os
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import PCA
import tarfile

from google.colab import drive
drive.mount('/content/drive')

"""Extract Dataset"""

def extract_tarfile(file_path, destination_path):
    with tarfile.open(file_path, 'r') as tar:
        tar.extractall(destination_path)

# Example usage
tar_file_path = '/content/drive/MyDrive/svarah.tar'
extract_to_directory = 'svarah'

extract_tarfile(tar_file_path, extract_to_directory)

"""Visualize Data"""

csv = pd.read_csv("/content/svarah/svarah/meta_speaker_stats.csv")
csv.head()

audio_csv = pd.DataFrame(csv.loc[:, 'audio_filepath'])

def extract_speaker_id(filepath):
    parts = filepath.split('/')
    speaker_id = parts[1].split('_')[0]
    return speaker_id

# Apply the function to all rows of the DataFrame
audio_csv['speaker_id'] = audio_csv['audio_filepath'].apply(extract_speaker_id)
speaker_ids = audio_csv['speaker_id'].unique()

audio_csv['speaker_id'] = audio_csv['audio_filepath'].str.split('_').str[0].str.split('/').str[1]

# Group by speaker ID and count occurrences
speaker_counts = audio_csv.groupby('speaker_id').size()

# Filter speaker IDs with more than 15 samples
subsampled_sid = speaker_counts[speaker_counts > 20].index.tolist()

print("Speaker IDs with more than 20 samples:")
print(subsampled_sid)

# Filter DataFrame to keep only rows with matching speaker IDs
sub_df = audio_csv[audio_csv['speaker_id'].isin(subsampled_sid)]

mapping = {val: idx + 1 for idx, val in enumerate(set(subsampled_sid))}

"""# Audio PreProcessing"""

import numpy as np
from scipy import signal

import numpy as np
import librosa
from scipy import signal

def preprocess_audio_mfcc(wav_file, sr=16000, n_fft=48, hop_length=256, n_mels=12):
  # Load audio using librosa (assumes WAV format)
  wav, _ = librosa.load(wav_file, sr=sr)

  # Frame division with overlap
  frames = librosa.util.frame(wav, frame_length=n_fft, hop_length=hop_length)

  # Apply window function (Hamming window by default)
  window = librosa.filters.get_window('hann', n_fft)
  window = window.reshape(-1, 1)  # Reshape to have one column
  windowed_frames = frames * window

  # FFT transformation
  fft_frames = np.fft.fft(windowed_frames, axis=1)
  # Compute MFCC features (using log-magnitude spectrum for stability)
  mfcc_features = librosa.feature.mfcc(y=np.abs(fft_frames), sr=sr, n_mels=n_mels, n_fft=n_fft)

  mfcc_features = np.mean(mfcc_features, axis=2)
  mfcc_features = np.mean(mfcc_features, axis=0)

  # Chroma_stft mean and variance
  chroma_mean = np.mean(librosa.feature.chroma_stft(y=wav, sr=sr), axis=1)
  chroma_var = np.var(librosa.feature.chroma_stft(y=wav, sr=sr), axis=1)

  # RMS mean and variance
  rms_mean = np.mean(librosa.feature.rms(y=wav))
  rms_var = np.var(librosa.feature.rms(y=wav))

  # Concatenate all features
  features = np.concatenate([mfcc_features , chroma_mean, chroma_var, [rms_mean], [rms_var]])

  return features

def extract_features(speaker_id=None, file=None):
  features = preprocess_audio_mfcc(file)

  filename = os.path.basename(file)

  df = pd.DataFrame(columns=['speaker_id'] + ['filename'] + [f'feature_{i}' for i in range(1, len(features)+1)])

  # Add data to DataFrame
  df.loc[0, 'speaker_id'] = speaker_id
  df.loc[0, 'filename'] = filename
  df.loc[0, 'feature_1':] = features

  return df

df_list = []

for i, sid in enumerate(subsampled_sid):
  audio_df = pd.DataFrame()
  speaker_df = sub_df[sub_df['speaker_id'] == sid]

  speaker_id = mapping.get(sid)

  # Iterate over filtered DataFrame and extract features from each audio file
  for index, row in speaker_df.iterrows():
    audio_file = os.path.join("/content/svarah/svarah", row['audio_filepath'])
    features = extract_features(speaker_id, audio_file)
    audio_df = pd.concat([audio_df, features], ignore_index=True)

  print(f"{i+1}/{len(subsampled_sid)} Extracted")
  df_list.append(audio_df)

"""## Split the dataset"""

from sklearn.model_selection import train_test_split

trained_dfs = []
test_dfs = []

for df in df_list:
  train, test = train_test_split(df, test_size=0.2, random_state=42, shuffle=False)
  train_df = pd.DataFrame(train)
  test_df = pd.DataFrame(test)

  trained_dfs.append(train_df)
  test_dfs.append(test_df)

"""## Train the model"""

import pandas as pd
from sklearn.mixture import GaussianMixture
import joblib

os.makedirs("/content/models", exist_ok=True)

save_dir = "/content/models"
# Function to train GMM for each speaker
def train_gmm_for_speakers(dfs, subsampled_sid):
    for df in trained_dfs:
        speaker_id = df['speaker_id'][0]
        mfcc_features = df.drop(columns=['filename', 'speaker_id'])  # Assuming 'speaker_id' is a column to drop

        # print(mfcc_features)

        # Train GMM
        gmm = GaussianMixture(n_components=10)  # Adjust the number of components as needed
        gmm.fit(mfcc_features)

        # Save trained GMM model
        model_filename = f'{save_dir}/{speaker_id}_gmm.pkl'
        joblib.dump(gmm, model_filename)
        print(f"GMM model saved for speaker ID {speaker_id} as {model_filename}")


# Train GMM for each speaker and save the models
train_gmm_for_speakers(trained_dfs, subsampled_sid)

"""## Evaluate"""

speakers_models = os.listdir("/content/models/")

fars = []
frrs = []

model_ids = []
eers = []

threshold = -38000000

for i, gdf in enumerate(test_dfs):
  frr, far = 0, 0

  genuine_speaker_id = gdf['speaker_id'].iloc[0]
  genuine_features = gdf.drop(columns=['filename', 'speaker_id'])

  model_ids.append(genuine_speaker_id)
  # print(genuine_speaker_id)
  for model in speakers_models:
    if str(genuine_speaker_id) in model:
      gmm = model
      break

  # Check for false rejection rate
  gmm = joblib.load(os.path.join("/content/models", gmm))

  # print(genuine_features.iloc[0])
  score = gmm.score_samples(genuine_features).mean()

  if score < threshold:
    frr += 1

  for j, idf in enumerate(test_dfs):
    if i != j:  # Skip the index of the first loop item
      imp_speaker_id = idf['speaker_id']
      imp_features = idf.drop(columns=['filename', 'speaker_id'])

       # print(genuine_features.iloc[0])
      iscore = gmm.score_samples(imp_features).mean()

      if iscore < threshold:
        far += 1

  far = far / (len(test_dfs) - 1)
  eer = (far + frr) / 2

  eers.append(eer)

  print(f"EER for {model}: {eer:0.4f}%")

import re
import matplotlib.pyplot as plt

indices = np.arange(len(model_ids))
# Create a bar plot
plt.figure(figsize=(7, 5))
plt.plot(indices, eers, marker='o', color='lightcoral', linestyle='-')
plt.xlabel('Model Index')
plt.ylabel('EER (%)')
plt.title('Equal Error Rates (EERs) for Different Models')
plt.xticks(indices, model_ids, rotation=90)  # Set model names as x-axis labels
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.grid(True)  # Add grid lines for better readability
plt.show()

"""## Authentication

# MAP
"""

def map_estimation(gmm_models, features):
    log_likelihood_speakers = np.zeros(len(gmm_models))
    model_names = []

    # Calculate log likelihoods for each speaker model
    for i, gmm in enumerate(gmm_models):

        if gmm.endswith(".pkl"):
          model_names.append(gmm)
          gmm = joblib.load(os.path.join("/content/models", gmm))
          scores = gmm.score(features)
          # print(scores)
          log_likelihood_speakers[i] = scores.sum()

    # Compute prior probabilities (assuming uniform priors)
    prior_probabilities = np.ones(len(gmm_models)) / len(gmm_models)

    # Compute posterior probabilities using Bayes' theorem
    log_posterior_probabilities = log_likelihood_speakers + np.log(prior_probabilities)

    # Get the predicted speaker ID with the maximum posterior probability
    predicted_index = np.argmax(log_posterior_probabilities)
    max_posterior_probability = log_posterior_probabilities[predicted_index]
    predicted_speaker = model_names[predicted_index].split("_")[0]

    return predicted_speaker


def predidc(filename):
  auth_audio_file = os.path.join("/content/svarah/svarah/audio", file_name)
  # auth_audio, _ = torchaudio.load(auth_audio_file)
  auth_ext_feat = extract_features(file=auth_audio_file)
  auth_ext_feat = auth_ext_feat.drop(columns=['filename', 'speaker_id'])
  prediction_speaker = map_estimation(speakers_models, auth_ext_feat)

  map = file_name.split("_")[0]

  return [prediction_speaker, mapping.get(map)]

file_name = "281474976883943_f2231_chunk_2.wav"
result = predidc(file_name)
print(f"Predicted Speaker ID: {result[0]}\nOriginal Speaker ID: {result[1]}")

file_name = "281474976888866_f2195_chunk_16.wav"
result = predidc(file_name)
print(f"Predicted Speaker ID: {result[0]}\nOriginal Speaker ID: {result[1]}")

"""## Enrollement"""

enroll_df = audio_csv[audio_csv['speaker_id']=="281474976895472"]
enroll_df.head()

enroll_id = max(mapping.values()) + 1

audio_df = pd.DataFrame()

for index, row in enroll_df.iterrows():
  audio_file = os.path.join("/content/svarah/svarah", row['audio_filepath'])

  features = extract_features(enroll_id, audio_file)
  audio_df = pd.concat([audio_df, features], ignore_index=True)

enroll_features = audio_df.drop(columns=['filename', 'speaker_id'])
# Train Gaussian Mixture Model (GMM)
num_components = 2 # Example number of components
gmm = GaussianMixture(n_components=num_components)
gmm.fit(enroll_features)

# Save the trained GMM model
joblib.dump(gmm, f'/content/models/{enroll_id}_gmm_model.pkl')